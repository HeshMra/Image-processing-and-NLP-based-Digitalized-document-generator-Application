{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, pickle, os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scrape import get_entire_web_google_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(file_dir):\n",
    "    df = None\n",
    "    for file in os.listdir(file_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            if df is None:\n",
    "                df = pd.read_csv(file_dir + file)\n",
    "                df = df[['Paragraph', 'Conclusion']]\n",
    "                df = df.dropna()\n",
    "            else:\n",
    "                df = df.append(pd.read_csv(file_dir + file))\n",
    "    \n",
    "    df = df.dropna(how='any', axis=0)\n",
    "    df['Paragraph'] = df['Paragraph'].apply(lambda x : x.lower().strip())\n",
    "    df['Conclusion'] = df['Conclusion'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "\n",
    "    paragraphs = df['Paragraph'].tolist()\n",
    "    conclusions = df['Conclusion'].tolist()\n",
    "\n",
    "    return paragraphs, conclusions\n",
    "\n",
    "def plot_token_length_distribution(paragraphs, conclusions):\n",
    "    paragraph_lengths = [len(paragraph.split()) for paragraph in paragraphs]\n",
    "    conclusion_lengths = [len(conclusion.split()) for conclusion in conclusions]\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(paragraph_lengths, bins=50)\n",
    "    plt.xlabel('paragraph Length')\n",
    "    plt.ylabel('Number of paragraphs')\n",
    "    plt.title('paragraph Length Distribution')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(conclusion_lengths, bins=50)\n",
    "    plt.xlabel('conclusion Length')\n",
    "    plt.ylabel('Number of conclusions')\n",
    "    plt.title('conclusion Length Distribution')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    b = [i for i in range(90,100)] \n",
    "    for i in b:  \n",
    "        print(i,'th percentile [paragraphS] is ', np.percentile(paragraph_lengths, i))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    for i in b:\n",
    "        print(i,'th percentile [conclusions] is ', np.percentile(conclusion_lengths, i))\n",
    "\n",
    "def get_tokenizer(\n",
    "                paragraphs, \n",
    "                conclusions,\n",
    "                save_path1,\n",
    "                save_path2\n",
    "                ):\n",
    "\n",
    "    tokenizer1 = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer1.fit_on_texts(paragraphs)\n",
    "\n",
    "    with open(save_path1, 'wb') as handle:\n",
    "        pickle.dump(tokenizer1, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    tokenizer2 = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer2.fit_on_texts(conclusions)\n",
    "\n",
    "    with open(save_path2, 'wb') as handle:\n",
    "        pickle.dump(tokenizer2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return tokenizer1, tokenizer2\n",
    "\n",
    "def process_dataset(\n",
    "                    file_dir,\n",
    "                    tokenizer_path_paragraphs,\n",
    "                    tokenizer_path_conclusions,\n",
    "                    max_length_paragraph = 500,\n",
    "                    max_length_conclusion = 150\n",
    "                    ):\n",
    "    paragraphs, conclusions = clean_dataset(file_dir)\n",
    "    plot_token_length_distribution(paragraphs, conclusions)\n",
    "\n",
    "    tokenizer_paragraphs, tokenizer_summarize = get_tokenizer(paragraphs, conclusions, tokenizer_path_paragraphs, tokenizer_path_conclusions)\n",
    "\n",
    "    paragraphs = tokenizer_paragraphs.texts_to_sequences(paragraphs) \n",
    "    conclusions = tokenizer_summarize.texts_to_sequences(conclusions)\n",
    "\n",
    "    paragraphs = tf.keras.preprocessing.sequence.pad_sequences(paragraphs, maxlen=max_length_paragraph, padding='post')\n",
    "    conclusions = tf.keras.preprocessing.sequence.pad_sequences(conclusions, maxlen=max_length_conclusion, padding='post')\n",
    "\n",
    "    return paragraphs, conclusions, tokenizer_paragraphs, tokenizer_summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16888\\1123840463.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m process_dataset(\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[1;34m'data/conclusion/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[1;34m'weights/conclusion/TOKENIZER_FACTS_MODEL_PARAGRAPHS.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;34m'weights/conclusion/TOKENIZER_FACTS_MODEL_CONCLUTIONS.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             )\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16888\\3808134138.py\u001b[0m in \u001b[0;36mprocess_dataset\u001b[1;34m(file_dir, tokenizer_path_paragraphs, tokenizer_path_conclusions, max_length_paragraph, max_length_conclusion)\u001b[0m\n\u001b[0;32m     76\u001b[0m                     \u001b[0mmax_length_conclusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                     ):\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mparagraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconclusions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mplot_token_length_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconclusions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16888\\3808134138.py\u001b[0m in \u001b[0;36mclean_dataset\u001b[1;34m(file_dir)\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'any'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \"\"\"\n\u001b[1;32m--> 347\u001b[1;33m     op = _Concatenator(\n\u001b[0m\u001b[0;32m    348\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    380\u001b[0m     ):\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mABCSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             raise TypeError(\n\u001b[0m\u001b[0;32m    383\u001b[0m                 \u001b[1;34m\"first argument must be an iterable of pandas \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                 \u001b[1;34mf'objects, you passed an object of type \"{type(objs).__name__}\"'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\""
     ]
    }
   ],
   "source": [
    "process_dataset(\n",
    "            'data/conclusion/',\n",
    "            'weights/conclusion/TOKENIZER_FACTS_MODEL_PARAGRAPHS.pkl',\n",
    "            'weights/conclusion/TOKENIZER_FACTS_MODEL_CONCLUTIONS.pkl',\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(max_x_len, x_voc_size):\n",
    "    encoder_inputs = tf.keras.layers.Input(shape=(max_x_len,))\n",
    "    enc_emb = tf.keras.layers.Embedding(x_voc_size, 300, mask_zero=True)(encoder_inputs)\n",
    "    encoder_lstm = tf.keras.layers.LSTM(300, return_sequences=True, return_state=True)\n",
    "    encoder_output, state_h, state_c = encoder_lstm(enc_emb)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    return encoder_inputs, encoder_states\n",
    "\n",
    "def decoder(max_y_len, y_voc_size, encoder_states):\n",
    "    decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "    dec_emb_layer = tf.keras.layers.Embedding(y_voc_size, 300, mask_zero=True)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "    decoder_lstm = tf.keras.layers.LSTM(300, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "    decoder_dense = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(y_voc_size, activation='softmax'))\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    return decoder_inputs, decoder_outputs\n",
    "\n",
    "def seq2seq(max_x_len, x_voc_size, max_y_len, y_voc_size):\n",
    "    encoder_inputs, encoder_states = encoder(max_x_len, x_voc_size)\n",
    "    decoder_inputs, decoder_outputs = decoder(max_y_len, y_voc_size, encoder_states)\n",
    "    model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(                   \n",
    "        file_dir,\n",
    "        tokenizer_path_paragraphs,\n",
    "        tokenizer_path_conclusions,\n",
    "        summarization_model_path\n",
    "        ):\n",
    "    \n",
    "    paragraphs, conclusions, tokenizer_paragraphs, tokenizer_summarize = process_dataset(file_dir, tokenizer_path_paragraphs, tokenizer_path_conclusions)\n",
    "    max_x_len = paragraphs.shape[1]\n",
    "    max_y_len = conclusions.shape[1]\n",
    "    x_voc_size = len(tokenizer_paragraphs.word_index) + 1\n",
    "    y_voc_size = len(tokenizer_summarize.word_index) + 1\n",
    "\n",
    "    model = seq2seq(max_x_len, x_voc_size, max_y_len, y_voc_size)\n",
    "\n",
    "    call_bactks = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "    model.compile(\n",
    "                optimizer='adam', \n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=[\n",
    "                        'accuracy',\n",
    "                        tf.keras.metrics.Precision(),\n",
    "                        tf.keras.metrics.Recall(),\n",
    "                        tf.keras.metrics.AUC()\n",
    "                        ])\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "            [paragraphs, conclusions[:,:-1]], \n",
    "            tf.keras.utils.to_categorical(conclusions.reshape(conclusions.shape[0], conclusions.shape[1], 1)[:,1:]), \n",
    "            batch_size=12, \n",
    "            epochs=1000,\n",
    "            callbacks=[call_bactks]\n",
    "            )\n",
    "    \n",
    "    model.save(summarization_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    'data/conclusion/',\n",
    "    'weights/conclusion/TOKENIZER_FACTS_MODEL_PARAGRAPHS.pkl',\n",
    "    'weights/conclusion/TOKENIZER_FACTS_MODEL_CONCLUTIONS.pkl',\n",
    "    'weights/conclusion/CONCLUTION_GENERATION_MODEL.h5'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "\"Jaykay Marketing Services (Pvt) Limited, part of the Keells Holdings PLC conglomerate, introduced the Keells Super to the Sri Lankan supermarket chain in 1991 by opening doors to the market as a new concept. Jaykay Marketing Services (Pvt) Limited is owned by huge companies in a variety of industries. Jaykay Marketing Services (Pvt.) Limited, which opened Keells Super's first location in Liberty Plaza, now has 109 locations all over the island, expanding the supply chain.\n",
    "The organization's essential providers included market pioneers like Ceylon Biscuits Limited and Unilever Sri Lanka Limited. Ceylon Colds Stores PLC, the parent company of Keells Supermarket, and Keells Food Products PLC, a related entity, are also Keells Supermarket's direct suppliers. However, when organic and own-label products were introduced, medium-sized suppliers and farmers joined the JMSL in sourcing.\n",
    "At first, Keells Super's primary business was the sale of a wide range of goods, primarily FMCG (fast-moving consumer goods), such as groceries, household goods, and other necessities that could satisfy the requirements of customers of the time. In 2000, a market trend prompted the development of the Nexus Mobile Loyalty Card system. Each bill received points that could be used later. Every supermarket began to provide payment options for utility bills and prescriptions over time.\n",
    "However, in order to meet the ever-increasing needs of its customers and keep up with market trends, the company opened its own bakery in there supermarkets, which led to an increase in sales. And in line with the general trend in the market, Jaykay Marketing Services (Pvt.) In December 2000, Limited introduced the Nexus Loyalty program with a points system that allowed customers to later redeem their bills, strengthening their relationship with Keells Supermarket.\n",
    "\"\"Our passion is to deliver pleasure and nutrition throughout people's lives, through exciting and superior products, whenever and wherever they choose to eat and drink,\"\" is the vision that they have passionate to make a quality difference in the world.\n",
    "Under the group practices of Jaykay Marketing Services (Pvt.) Limited it recreate it`s goals and objectives once every 5 years to meet the customers` needs and ultimately to ensure the success of the business process. As part of the company's effort to increase revenue, Keells supermarket owns over 300 products under its own brand name, \"\"K Choice.\"\" \"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path_paragraphs = 'weights/conclusion/TOKENIZER_FACTS_MODEL_PARAGRAPHS.pkl'\n",
    "tokenizer_path_conclusions = 'weights/conclusion/TOKENIZER_FACTS_MODEL_CONCLUTIONS.pkl'\n",
    "conclution_generation_model_path = 'weights/conclusion/CONCLUTION_GENERATION_MODEL.h5'\n",
    "\n",
    "with open(tokenizer_path_paragraphs, 'rb') as handle:\n",
    "    tokenizer_paragraphs = pickle.load(handle)\n",
    "\n",
    "with open(tokenizer_path_conclusions, 'rb') as handle:\n",
    "    tokenizer_conclusions = pickle.load(handle)\n",
    "\n",
    "encoder_inputs, encoder_states = encoder(500, len(tokenizer_paragraphs.word_index) + 1)\n",
    "decoder_inputs, decoder_outputs = decoder(150, len(tokenizer_conclusions.word_index) + 1, encoder_states)\n",
    "inference_model = tf.keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "inference_model.load_weights(conclution_generation_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_conclution_generation(\n",
    "                                    input_text,\n",
    "                                    max_x_len = 500,\n",
    "                                    max_y_len = 150\n",
    "                                    ):\n",
    "\n",
    "    input_text = tokenizer_paragraphs.texts_to_sequences([input_text])\n",
    "    input_text = tf.keras.preprocessing.sequence.pad_sequences(input_text, maxlen=max_x_len, padding='post')\n",
    "\n",
    "    conclusion = np.zeros((1, max_y_len))\n",
    "    conclusion[0,0] = tokenizer_conclusions.word_index['sostok']\n",
    "    stop_condition = False\n",
    "    i = 1\n",
    "    while not stop_condition:\n",
    "        preds = inference_model.predict([input_text, conclusion], verbose=0)\n",
    "        pred = np.argmax(preds[0,i-1])\n",
    "        conclusion[0,i] = pred\n",
    "        i += 1\n",
    "        if pred == tokenizer_conclusions.word_index['eostok'] or i >= max_y_len:\n",
    "            stop_condition = True\n",
    "\n",
    "    conclusion = conclusion[0]\n",
    "    new_conclusion = []\n",
    "    for i in conclusion:\n",
    "        if i != 0:\n",
    "            new_conclusion.append(i)\n",
    "    conclusion = ' '.join([tokenizer_conclusions.index_word[i] for i in new_conclusion])\n",
    "    conclusion = conclusion.replace('eostok', '').replace('sostok', '').strip()\n",
    "    return conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conclusion_facts = inference_conclution_generation(text)\n",
    "conclusion_facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_references(search_item):\n",
    "    df = get_entire_web_google_results(search_item)\n",
    "    image_urls = df['image_url'].tolist()\n",
    "    image_urls = [i for i in image_urls if i != '']\n",
    "    image_urls = [i for i in image_urls if 'logo' not in i]\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_item = '''A printer is a device that produces text and graphics on paper or other materials. There are several types of printers available, including inkjet printers, laser printers, and thermal printers. Inkjet printers are popular for home use, while laser printers are commonly used in offices due to their speed and durability. Thermal printers are commonly used for printing receipts or shipping labels. Printers can be connected to computers, mobile devices, or network servers, and can be used to print a wide range of documents, including photos, reports, and marketing materials. Advances in printer technology have led to increased efficiency and lower costs, making them a crucial tool in modern business and personal use.'''\n",
    "image_urls = scraping_references(search_item)\n",
    "image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
