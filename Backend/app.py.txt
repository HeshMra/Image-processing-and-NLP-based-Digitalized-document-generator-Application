import time
import os,re
import spacy
import boto3
import datetime
import pickle, io
import numpy as np
import pandas as pd
from fpdf import FPDF
from gtts import gTTS
import tensorflow as tf
import torch, whisper, glob
from google.cloud import vision
import torch.nn.functional as F
import speech_recognition as sr
from whisper.model import Whisper
from langchain.llms import OpenAI
from langchain import PromptTemplate
from langchain.chains import LLMChain
from typing import List, Optional, Dict
from google.cloud.vision_v1.types import Image
from scrape import get_entire_web_google_results
from google.oauth2.service_account import Credentials
from whisper.tokenizer import Tokenizer, get_tokenizer
from whisper.audio import N_FRAMES, N_MELS, log_mel_spectrogram, pad_or_trim
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline

from flask_cors import CORS
from werkzeug.utils import secure_filename
from flask import Flask, request, jsonify, Response

os.environ["OPENAI_API_KEY"] = "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"
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads/'
CORS(app)

s3_client = boto3.client('s3',
                        aws_access_key_id='AKIASJHMNZBZM3AQTUFZ',
                        aws_secret_access_key='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'
                        )

s3_resource = boto3.resource('s3',
                            aws_access_key_id='AKIASJHMNZBZM3AQTUFZ',
                            aws_secret_access_key='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'
                            )                             
                    
bucket = 'drivecaremobile7515b7630b154bd18deba55c989120d7213432-dev'
S3url = 'https://drivecaremobile7515b7630b154bd18deba55c989120d7213432-dev.s3.amazonaws.com/'

def UploadS3(local_video_path, s3_video_path):
    s3_resource.meta.client.upload_file(local_video_path, bucket, s3_video_path)

######################## COMPONENT 1 VARIABLES ########################
nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')
classifier = pipeline(
                    "zero-shot-classification",
                    model="facebook/bart-large-mnli",
                    device = 0 if torch.cuda.is_available() else -1
                    )
creds = Credentials.from_service_account_file('data/ocr/credentials.json')
client = vision.ImageAnnotatorClient(credentials=creds)

candidates = [
                'Create_and_submit_Topic_Assesment_Form',
                'Create_and_submit_Charter_Form',
                'Create_and_submit_proposal_draft',
                'Final_report_creation_submission'
                ]

######################## COMPONENT 2 VARIABLES ########################
llm_quote_reference = OpenAI(
                            model_name='gpt-3.5-turbo', 
                            temperature=0, 
                            max_tokens = 256
                            )

prompt_quote_reference = '''
                        Please provide me the most suitable Web URL from this list of below websites which contains quote; "{quote}" in below format from below URLs,
                        Websites need to be scraped: [
                                                    'https://en.wikipedia.org/wiki/',
                                                    'https://scholar.google.com/',
                                                    'https://www.researchgate.net/',
                                                    'https://ieeexplore.ieee.org/Xplore/home.jsp',
                                                    'https://www.encyclopedia.com/',
                                                    'https://dl.acm.org/',
                                                    'https://www.sciencedirect.com/'
                                                    ]
                                                    
                        Format : url
                    '''

prompt_quote_reference_template = PromptTemplate(
                                                input_variables=["quote"],
                                                template=prompt_quote_reference
                                                )
llm_chain_quote = LLMChain(
                        llm=llm_quote_reference,
                        prompt=prompt_quote_reference_template
                        )

######################## COMPONENT 3 VARIABLES ########################
llm_conclsion = OpenAI(
                        model_name='gpt-3.5-turbo', 
                        temperature=0,
                        max_tokens=600,
                        )

prompt_conclsion = '''can you write 400 words  introduction within single paragraph  regarding the Paragraph; {paragraph}'''

prompt_conclsion_template = PromptTemplate(
                                        input_variables=["paragraph"],
                                        template=prompt_conclsion
                                        )
llm_chain_conclsion = LLMChain(
                            llm=llm_conclsion,
                            prompt=prompt_conclsion_template
                            )

###################################################################################################################################
###################################################################################################################################
###################################################################################################################################
###################################################################################################################################


################################ COMPONENT 1 UTILITY FUNCTIONS ################################
def detect_handwritten_text(path):
    with io.open(path, 'rb') as image_file:
        content = image_file.read()

    image = Image(content=content)
    response = client.document_text_detection(image=image)
    if response.error.message:
        raise Exception(
            '{}\nFor more info on error messages, check: '
            'https://cloud.google.com/apis/design/errors'.format(
                response.error.message))
    
    page = response.full_text_annotation.pages[0]
    parapgraph_texts = ''
    for block in page.blocks:
        parapgraph_text = ''
        for paragraph in block.paragraphs:
            for word in paragraph.words:
                word_text = ''.join([
                    symbol.text for symbol in word.symbols
                ])
                parapgraph_text += word_text + ' '
        parapgraph_texts += parapgraph_text + ' '
    return parapgraph_texts

def detect_gantt_chart(path):
    with io.open(path, 'rb') as image_file:
        content = image_file.read()

    image = Image(content=content)
    response = client.document_text_detection(image=image)
    if response.error.message:
        raise Exception(
            '{}\nFor more info on error messages, check: '
            'https://cloud.google.com/apis/design/errors'.format(
                response.error.message))
    
    page = response.full_text_annotation.pages[0]
    parapgraph_texts = []
    for block in page.blocks:
        parapgraph_text = ''
        for paragraph in block.paragraphs:
            for word in paragraph.words:
                word_text = ''.join([
                    symbol.text for symbol in word.symbols
                ])
                parapgraph_text += word_text + ' '
        parapgraph_texts.append(parapgraph_text)

    deadlines = parapgraph_texts[-4:]
    tasks = parapgraph_texts[1:-5]

    task_dict = {}
    for i in range(len(tasks)):
        task_dict[tasks[i].strip()] = deadlines[i].strip()
    return task_dict

def zero_shot_text_classification(corrected_text):
    try:
        tokenized_products = tokenizer.encode_plus(
                                                    corrected_text, 
                                                    return_tensors='pt', 
                                                    max_length=len(corrected_text.split()), 
                                                    pad_to_max_length=True
                                                    )
        preditions = nli_model(**tokenized_products)
        logits = preditions.logits
        probs = torch.softmax(logits, dim=-1)
        probs = probs.detach().numpy()
        probs = np.array(probs, dtype=np.object)
        probs = probs[:, 1]
        output = candidates[np.argmax(probs)]
    except:
        predition = classifier(corrected_text, candidates, multi_label=True)
        output = predition['labels'][np.argmax(predition['scores'])]
    return output

def write_to_pdf(text, task_type, pdf=None):
    pdf_args = pdf
    if pdf is None:
        pdf = FPDF()
    pdf.add_page()

    if pdf_args is None:
        pdf.set_font("Arial", size = 25)
        pdf.cell(
                200, 10, 
                txt = ' '.join(task_type.split('_')),
                ln = 1, 
                align = 'C'
                )
    
    text = text.split()
    pdf.set_font("Arial", size = 12)

    max_characters_per_line = 100
    curr_line = ''
    for i in range(len(text)):
        if len(curr_line) + len(text[i]) > max_characters_per_line:
            pdf.cell(200, 10, txt = curr_line, ln = 1, align = 'L')
            curr_line = text[i] + ' '
        else:
            curr_line += text[i] + ' '


    return pdf

def extract_single_image_content(image_path):
    text = detect_handwritten_text(image_path)
    task_type = zero_shot_text_classification(text)
    write_to_pdf(text, task_type)
    return text, task_type

def component_01_pipeline(
                        hand_written_image_paths,
                        gantt_chart_image_path,
                        digidoc_path = "store/ocr/digidoc.pdf",
                        read_out_loud_path = "store/ocr/read_out_loud.mp3",
                        digidoc_path_s3 = "data/ocr/digidoc.pdf",
                        read_out_loud_path_s3 = "data/ocr/read_out_loud.mp3"
                        ):
    texts = []
    task_types = []

    for image_path in hand_written_image_paths:
        text, task_type = extract_single_image_content(image_path)
        texts.append(text)
        task_types.append(task_type)

    mode_task_type = max(set(task_types), key=task_types.count)
    for idx, text in enumerate(texts):
        if idx == 0:
            pdf = write_to_pdf(text, mode_task_type)
        else:
            pdf = write_to_pdf(text, mode_task_type, pdf=pdf)
        
    pdf.output(digidoc_path)

    gantt_chart_dict = detect_gantt_chart(gantt_chart_image_path)
    deadline = gantt_chart_dict[mode_task_type]

    today = datetime.date.today()
    deadline = datetime.datetime.strptime(deadline, '%d/%m/%Y').date()
    daysleft = deadline - today

    readable_text = 'The Task Type for provided handwritten images is ' + ' '.join(mode_task_type.split('_')) + '.\n'
    for idx, text in enumerate(texts):
        readable_text += 'Reading Page ' + str(idx + 1) + ':\n'
        readable_text += text + '\n\n'

    readable_text += 'The deadline for the task is ' + deadline.strftime('%d/%m/%Y') + '.\n'
    readable_text += 'You have ' + str(daysleft.days) + ' days left to complete the task.\n'
    read_obj = gTTS(
                    text=readable_text, 
                    lang='en', 
                    slow=False
                    )
    read_obj.save(read_out_loud_path)

    UploadS3(digidoc_path, digidoc_path_s3)
    UploadS3(read_out_loud_path, read_out_loud_path_s3)

    return deadline.strftime('%d/%m/%Y'), daysleft.days, f"{S3url}{digidoc_path_s3}", f"{S3url}{read_out_loud_path_s3}"

################################ COMPONENT 2 UTILITY FUNCTIONS ################################
def Speech2text(audio_file):
    r = sr.Recognizer()
    with sr.AudioFile(audio_file) as source:
        audio = r.record(source)
    try:
        text = r.recognize_google(audio)
        return text
    except:
        return "Error"
    
def abbreviation_regex():
    pattern = r'\b([A-Za-z0-9_-]+)\b\s*\((\b[A-Za-z\s]+\b)\)'
    return pattern

def inference_abb(text):
    regex_pattern = abbreviation_regex()
    matches = re.findall(regex_pattern, text)
    
    abbr_list = []
    for match in matches:
        Abbreviation = match[0].strip().lower()
        Description = match[1].strip().lower()

        # verify if the abbreviation 
        Abbreviation_Derived = ''.join([word[0] for word in Description.split()])
        if Abbreviation_Derived == Abbreviation:
            abbr_json = {}
            abbr_json['Abbreviation'] = Abbreviation
            abbr_json['Description'] = Description
            abbr_list.append(abbr_json)
    
    return abbr_list

def inference_quote(text):
    time.sleep(15)
    response = llm_chain_quote.run(text)
    try:
        response = response.split('\n')[1].strip()
    except:
        response = response.strip()
    return response

################################ COMPONENT 3 UTILITY FUNCTIONS ################################

def inference_conclsion(text):
    time.sleep(15)
    text = text.replace('\n', ' ').strip()
    response = llm_chain_conclsion.run(text)
    return response.strip()

def scraping_images(search_item):
    df = get_entire_web_google_results(search_item)
    image_urls = df['image_url'].tolist()
    image_urls = [i for i in image_urls if i != '']
    image_urls = [i for i in image_urls if 'logo' not in i]
    return image_urls

################################ COMPONENT 4 UTILITY FUNCTIONS ################################

@torch.no_grad()
def calculate_audio_features(audio_path: Optional[str], model: Whisper) -> torch.Tensor:
    if audio_path is None:
        segment = torch.zeros((N_MELS, N_FRAMES), dtype=torch.float32).to(model.device)
    else:
        mel = log_mel_spectrogram(audio_path)
        segment = pad_or_trim(mel, N_FRAMES).to(model.device)
    return model.embed_audio(segment.unsqueeze(0))


@torch.no_grad()
def calculate_average_logprobs(
                                model: Whisper,
                                audio_features: torch.Tensor,
                                class_names: List[str],
                                tokenizer: Tokenizer,
                            ) -> torch.Tensor:
    initial_tokens = (
        torch.tensor(tokenizer.sot_sequence_including_notimestamps).unsqueeze(0).to(model.device)
    )
    eot_token = torch.tensor([tokenizer.eot]).unsqueeze(0).to(model.device)

    average_logprobs = torch.zeros(len(class_names))
    for i, class_name in enumerate(class_names):
        class_name_tokens = (
            torch.tensor(tokenizer.encode(" " + class_name)).unsqueeze(0).to(model.device)
        )
        input_tokens = torch.cat([initial_tokens, class_name_tokens, eot_token], dim=1)

        logits = model.logits(input_tokens, audio_features)  # (1, T, V)
        logprobs = F.log_softmax(logits, dim=-1).squeeze(0)  # (T, V)
        logprobs = logprobs[len(tokenizer.sot_sequence_including_notimestamps) - 1 : -1]  # (T', V)
        logprobs = torch.gather(logprobs, dim=-1, index=class_name_tokens.view(-1, 1))  # (T', 1)
        average_logprob = logprobs.mean().item()
        average_logprobs[i] = average_logprob

    return average_logprobs


def calculate_internal_lm_average_logprobs(
                                            model: Whisper,
                                            class_names: List[str],
                                            tokenizer: Tokenizer,
                                            verbose: bool = False,
                                        ) -> torch.Tensor:
    audio_features_from_empty_input = calculate_audio_features(None, model)
    average_logprobs = calculate_average_logprobs(
                                                model=model,
                                                audio_features=audio_features_from_empty_input,
                                                class_names=class_names,
                                                tokenizer=tokenizer,
                                                )
    if verbose:
        print("Internal LM average log probabilities for each class:")
        for i, class_name in enumerate(class_names):
            print(f"  {class_name}: {average_logprobs[i]:.3f}")
    return average_logprobs

model_cache = {}

def generate_candidate_str(command_path = 'data/voice_command/commands.txt'):
    with open(command_path, 'r') as f:
        commands = f.readlines()
    commands = [c.strip() for c in commands]
    commands = [c for c in commands if len(c) > 0]
    commands = [f"[{c}]" for c in commands]
    return ','.join(commands)

def zero_shot_classify(
                        audio_path: str, 
                        model_name = 'small'
                        ) -> Dict[str, float]:
    class_names = generate_candidate_str()
    class_names = class_names.split(",")
    tokenizer = get_tokenizer(multilingual=".en" not in model_name)

    if model_name not in model_cache:
        model = whisper.load_model(model_name)
        model_cache[model_name] = model
    else:
        model = model_cache[model_name]

    internal_lm_average_logprobs = calculate_internal_lm_average_logprobs(
        model=model,
        class_names=class_names,
        tokenizer=tokenizer,
    )
    audio_features = calculate_audio_features(audio_path, model)
    average_logprobs = calculate_average_logprobs(
        model=model,
        audio_features=audio_features,
        class_names=class_names,
        tokenizer=tokenizer,
    )
    average_logprobs -= internal_lm_average_logprobs
    scores = average_logprobs.softmax(-1).tolist()
    return {class_name: score for class_name, score in zip(class_names, scores)}

def extract_command(file_name):
    return file_name.split('/')[-1]

def process_output(output):
    command = max(output, key=output.get)
    command = command[1:-1]
    return command

def inference_voice_command(audio_file):
    result = zero_shot_classify(audio_file)
    result = process_output(result)
    return result

def scraping_references(search_item):
    df = get_entire_web_google_results(search_item)
    df = df[['title', 'description', 'DOI']]
    return df.to_dict('records')

#################################################################################################################################
#################################################################################################################################
#################################################################################################################################

# COMPONENT 1 [DIGI DOC]
@app.route('/digi_doc', methods=['POST'])
def digi_doc():
    if request.method == 'POST':
        hand_written_images = []
        for i in range(1, 5):
            hand_written_image_i = request.files[f'hand_written_image_{i}']
            filename = secure_filename(hand_written_image_i.filename)
            hand_written_image_i.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))
            hand_written_images.append(os.path.join(app.config['UPLOAD_FOLDER'], filename))

        gantt_chart_image = request.files['gantt_chart_image']
        filename = secure_filename(gantt_chart_image.filename)
        gantt_chart_image.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))
        gantt_chart_image = os.path.join(app.config['UPLOAD_FOLDER'], filename)

        deadline, daysleft, digidoc_path, read_out_loud_path = component_01_pipeline(hand_written_images, gantt_chart_image)

        return jsonify({
                        "deadline" : f"{deadline}",
                        "daysleft" : f"{daysleft}",
                        "digidoc_path" : f"{digidoc_path}",
                        "read_out_loud_path" : f"{read_out_loud_path}"
                    })
    
    return jsonify({
                    "deadline" : "No file uploaded",
                    "daysleft" : "No file uploaded",
                    "digidoc_path" : "No file uploaded",
                    "read_out_loud_path" : "No file uploaded"
                })


# COMPONENT 2 [SPEECH TO TEXT]
@app.route('/speech_to_text', methods=['POST'])
def speech_to_text():
    if request.method == 'POST':
        file = request.files['file']
        filename = secure_filename(file.filename)
        file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))

        audio_file = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        result = Speech2text(audio_file)

        return jsonify({
                        "text" : f"{result}"
                    })
    
    return jsonify({
                    "text" : "No file uploaded"
                })

# COMPONENT 2 [ABBREVIATION AND REFERENCES]
@app.route('/abbreviation_and_references', methods=['POST'])
def abbreviation_and_references():
    if request.method == 'POST':
        text = request.form['text']
        abbr_list = inference_abb(text)
        reference_quote = inference_quote(text)

        return jsonify({
                        "abbreviation" : abbr_list,
                        "reference_quote" : f"{reference_quote}"
                    })
    
    return jsonify({
                    "abbreviation" : "No text uploaded",
                    "reference_quote" : "No text uploaded"
                })

# COMPONENT 3 [CONCLUSION AND REFERENCES]
@app.route('/conclusion_and_references', methods=['POST'])
def conclusion_and_references():
    if request.method == 'POST':
        text = request.form['text']
        conclusion = inference_conclsion(text)
        references = scraping_images(text)

        return jsonify({
                        "image_refs" : references,
                        "conclusion" : f"{conclusion}"
                    })
    
    return jsonify({
                    "image_refs" : "No text uploaded",
                    "conclusion" : "No text uploaded"
                })

# COMPONENT 4 [VOICE COMMANDS]
@app.route('/voice_commands', methods=['POST'])
def voice_commands():
    if request.method == 'POST':
        audio_file = request.files['file']
        filename = secure_filename(audio_file.filename)
        audio_file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))

        result = inference_voice_command(os.path.join(app.config['UPLOAD_FOLDER'], filename))

        return jsonify({
                        "command" : f"{result}"
                    })
    
    return jsonify({
                    "command" : "No file uploaded"
                })

# COMPONENT 4 [REFERENCES]
@app.route('/references', methods=['POST'])
def references():
    if request.method == 'POST':
        text = request.form['text']
        references = scraping_references(text)

        return jsonify({
                        "references" : references
                    })
    
    return jsonify({
                    "references" : "No text uploaded"
                })

if __name__ == '__main__':
    app.run(
            host="0.0.0.0",
            port=5000,
            debug=True
            )
