{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datefinder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12836\\758159953.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdatefinder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0measyocr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlanguage_tool_python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datefinder'"
     ]
    }
   ],
   "source": [
    "import datefinder\n",
    "import numpy as np\n",
    "import easyocr, torch\n",
    "import language_tool_python\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "classifier = pipeline(\n",
    "                    \"zero-shot-classification\",\n",
    "                    model=\"facebook/bart-large-mnli\",\n",
    "                    device = 0 if torch.cuda.is_available() else -1\n",
    "                    )\n",
    "\n",
    "candidates = [\n",
    "            'Topic selection & registration',\n",
    "            'Create & submit Topic assesment form',\n",
    "            'Create & submit Charter form',\n",
    "            'Create & submit proposal draft',\n",
    "            'Proposal Presenation',\n",
    "            'Identify process deliverables',\n",
    "            'Select technologies',\n",
    "            'Collecting the data set',\n",
    "            'Training relevant data sets',\n",
    "            'Design wireframes',\n",
    "            'Database design',\n",
    "            'Backend development',\n",
    "            'Frontend development',\n",
    "            'Unit test',\n",
    "            'Integration test' ,\n",
    "            'QA test',\n",
    "            'Working on exsisting bugs',\n",
    "            'Fix bugs',\n",
    "            'Finalize the system',\n",
    "            'Final report creation & submission',\n",
    "            'Final Presentation',\n",
    "            'Releasing Phase',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extraction(image_path):\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    result = reader.readtext(image_path, detail = 0)\n",
    "    result = ' '.join(result)\n",
    "    matches = tool.check(result)\n",
    "    corrected = language_tool_python.utils.correct(result, matches)\n",
    "    return corrected\n",
    "\n",
    "def zero_shot_text_classification(corrected_text):\n",
    "    try:\n",
    "        tokenized_products = tokenizer.encode_plus(\n",
    "                                                    corrected_text, \n",
    "                                                    return_tensors='pt', \n",
    "                                                    max_length=len(corrected_text.split()), \n",
    "                                                    pad_to_max_length=True\n",
    "                                                    )\n",
    "        preditions = nli_model(**tokenized_products)\n",
    "        logits = preditions.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        probs = probs.detach().numpy()\n",
    "        probs = np.array(probs, dtype=np.object)\n",
    "        probs = probs[:, 1]\n",
    "        output = candidates[np.argmax(probs)]\n",
    "    except:\n",
    "        predition = classifier(corrected_text, candidates, multi_label=True)\n",
    "        output = predition['labels'][np.argmax(predition['scores'])]\n",
    "    return output\n",
    "\n",
    "def extract_date(text):\n",
    "    matches = datefinder.find_dates(text)\n",
    "    for match in matches:\n",
    "        print(match)\n",
    "\n",
    "def component_01_pipeline(image_path):\n",
    "    corrected_text = text_extraction(image_path)\n",
    "    zero_shot_text_classification(corrected_text)\n",
    "    print(\"This Ducemnt is about: \", zero_shot_text_classification(corrected_text))\n",
    "    # extract_date(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Legion\\.conda\\envs\\torch113\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\Legion\\AppData\\Local\\Temp\\ipykernel_7028\\629145146.py:21: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  probs = np.array(probs, dtype=np.object)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Ducemnt is about:  Topic selection & registration\n"
     ]
    }
   ],
   "source": [
    "image_path = 'data/ocr/HandWrittenData/1.jpg'\n",
    "component_01_pipeline(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
