{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "from pytorch_detection import utils\n",
    "import pytorch_detection.transforms as T\n",
    "from pytorch_detection.engine import train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "class OCRGanttDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.data_dir = os.path.join(self.root, 'ocr-od')\n",
    "        self.classes = [\n",
    "                        'deployment', \n",
    "                        'App Design', \n",
    "                        'Project Planning', \n",
    "                        'Future Enhancement', \n",
    "                        'Back-end dev', \n",
    "                        'Front-end Development', \n",
    "                        'tesing and QA', \n",
    "                        'Deadline', \n",
    "                        'Maintainance', \n",
    "                        'Back-end development'\n",
    "                        ]\n",
    "\n",
    "        self.imgs = glob.glob(os.path.join(self.data_dir, '*.jpg'))\n",
    "        self.annotations = glob.glob(os.path.join(self.data_dir, '*.json'))\n",
    "\n",
    "        print(\"Number of images: \", len(self.imgs))\n",
    "        print(\"Number of annotations: \", len(self.annotations))\n",
    "\n",
    "        all_labels = []\n",
    "        for annotation_file in self.annotations:\n",
    "            with open(annotation_file) as f:\n",
    "                annotation_data = json.load(f)\n",
    "                annotation_data = annotation_data[\"shapes\"]\n",
    "                for annotation in annotation_data:\n",
    "                    annotation_label = annotation[\"label\"]\n",
    "                    all_labels.append(annotation_label)\n",
    "        print(\"All labels: \", list(set(all_labels)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and annotations\n",
    "        image_file = self.imgs[idx]\n",
    "        annotation_file = self.annotations[idx]\n",
    "\n",
    "        img_path = image_file.replace(\"\\\\\", \"/\").replace(\"//\", \"/\")\n",
    "        annotation_path = annotation_file.replace(\"\\\\\", \"/\").replace(\"//\", \"/\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(annotation_path) as f:\n",
    "            annotation_data = json.load(f)\n",
    "            annotation_data = annotation_data[\"shapes\"]\n",
    "            for annotation in annotation_data:\n",
    "                annotation_bbox = annotation[\"points\"]\n",
    "                annotation_label = annotation[\"label\"]\n",
    "                x1, y1 = annotation_bbox[0]\n",
    "                x2, y2 = annotation_bbox[1]\n",
    "\n",
    "                if (x1 > x2):\n",
    "                    x1, x2 = x2, x1\n",
    "                if (y1 > y2):\n",
    "                    y1, y2 = y2, y1\n",
    "\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels.append(self.classes.index(annotation_label))\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        image_id = torch.tensor([idx])\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  49\n",
      "Number of annotations:  49\n",
      "All labels:  ['deployment', 'App Design', 'Project Planning', 'Future Enhancement', 'Back-end dev', 'Front-end Development', 'tesing and QA', 'Deadline', 'Maintainance', 'Back-end development']\n",
      "CONSUMING CPU\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "dataset  = OCRGanttDataSet(root=\"data\", transforms=get_transform(train=True))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "                                        dataset, \n",
    "                                        batch_size=2, \n",
    "                                        shuffle=True, \n",
    "                                        collate_fn=utils.collate_fn\n",
    "                                        )\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device('cpu')\n",
    "print(\"CONSUMING GPU\" if device == torch.device('cuda') else \"CONSUMING CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/25]  eta: 0:00:24  lr: 0.000213  loss: 3.8630 (3.8630)  loss_classifier: 2.3937 (2.3937)  loss_box_reg: 0.0770 (0.0770)  loss_objectness: 1.1723 (1.1723)  loss_rpn_box_reg: 0.2200 (0.2200)  time: 0.9982  data: 0.0400  max mem: 0\n",
      "Epoch: [0]  [10/25]  eta: 0:00:23  lr: 0.002294  loss: 1.9089 (2.3025)  loss_classifier: 1.2971 (1.4511)  loss_box_reg: 0.2964 (0.2432)  loss_objectness: 0.1451 (0.4542)  loss_rpn_box_reg: 0.1263 (0.1540)  time: 1.5987  data: 0.0195  max mem: 0\n",
      "Epoch: [0]  [20/25]  eta: 0:00:08  lr: 0.004376  loss: 1.2268 (1.6657)  loss_classifier: 0.5034 (0.9675)  loss_box_reg: 0.3018 (0.2694)  loss_objectness: 0.1288 (0.2885)  loss_rpn_box_reg: 0.1061 (0.1402)  time: 1.7751  data: 0.0173  max mem: 0\n",
      "Epoch: [0]  [24/25]  eta: 0:00:01  lr: 0.005000  loss: 1.0209 (1.5610)  loss_classifier: 0.4534 (0.8947)  loss_box_reg: 0.3239 (0.2879)  loss_objectness: 0.0968 (0.2508)  loss_rpn_box_reg: 0.0933 (0.1276)  time: 1.8279  data: 0.0167  max mem: 0\n",
      "Epoch: [0] Total time: 0:00:43 (1.7204 s / it)\n",
      "Epoch: [1]  [ 0/25]  eta: 0:00:36  lr: 0.005000  loss: 1.3890 (1.3890)  loss_classifier: 0.6825 (0.6825)  loss_box_reg: 0.5897 (0.5897)  loss_objectness: 0.0250 (0.0250)  loss_rpn_box_reg: 0.0917 (0.0917)  time: 1.4442  data: 0.0170  max mem: 0\n",
      "Epoch: [1]  [10/25]  eta: 0:00:25  lr: 0.005000  loss: 1.0382 (1.1158)  loss_classifier: 0.5343 (0.5404)  loss_box_reg: 0.4153 (0.4498)  loss_objectness: 0.0572 (0.0517)  loss_rpn_box_reg: 0.0702 (0.0739)  time: 1.7122  data: 0.0166  max mem: 0\n",
      "Epoch: [1]  [20/25]  eta: 0:00:08  lr: 0.005000  loss: 1.0281 (1.0779)  loss_classifier: 0.4828 (0.5033)  loss_box_reg: 0.4127 (0.4434)  loss_objectness: 0.0499 (0.0515)  loss_rpn_box_reg: 0.0764 (0.0797)  time: 1.6591  data: 0.0163  max mem: 0\n",
      "Epoch: [1]  [24/25]  eta: 0:00:01  lr: 0.005000  loss: 0.9727 (1.0522)  loss_classifier: 0.4423 (0.4839)  loss_box_reg: 0.4127 (0.4375)  loss_objectness: 0.0441 (0.0494)  loss_rpn_box_reg: 0.0772 (0.0814)  time: 1.6481  data: 0.0159  max mem: 0\n",
      "Epoch: [1] Total time: 0:00:41 (1.6549 s / it)\n",
      "Epoch: [2]  [ 0/25]  eta: 0:00:48  lr: 0.005000  loss: 0.7554 (0.7554)  loss_classifier: 0.3514 (0.3514)  loss_box_reg: 0.2678 (0.2678)  loss_objectness: 0.0388 (0.0388)  loss_rpn_box_reg: 0.0974 (0.0974)  time: 1.9234  data: 0.0210  max mem: 0\n",
      "Epoch: [2]  [10/25]  eta: 0:00:25  lr: 0.005000  loss: 0.9171 (0.9130)  loss_classifier: 0.4124 (0.4130)  loss_box_reg: 0.4019 (0.3785)  loss_objectness: 0.0381 (0.0427)  loss_rpn_box_reg: 0.0745 (0.0788)  time: 1.6884  data: 0.0169  max mem: 0\n",
      "Epoch: [2]  [20/25]  eta: 0:00:08  lr: 0.005000  loss: 0.9171 (0.9260)  loss_classifier: 0.4091 (0.4181)  loss_box_reg: 0.4000 (0.3890)  loss_objectness: 0.0445 (0.0445)  loss_rpn_box_reg: 0.0692 (0.0744)  time: 1.6636  data: 0.0164  max mem: 0\n",
      "Epoch: [2]  [24/25]  eta: 0:00:01  lr: 0.005000  loss: 0.9436 (0.9580)  loss_classifier: 0.4124 (0.4305)  loss_box_reg: 0.4059 (0.4082)  loss_objectness: 0.0449 (0.0459)  loss_rpn_box_reg: 0.0681 (0.0734)  time: 1.6085  data: 0.0167  max mem: 0\n",
      "Epoch: [2] Total time: 0:00:40 (1.6235 s / it)\n",
      "Epoch: [3]  [ 0/25]  eta: 0:00:41  lr: 0.000500  loss: 1.0659 (1.0659)  loss_classifier: 0.4365 (0.4365)  loss_box_reg: 0.5336 (0.5336)  loss_objectness: 0.0202 (0.0202)  loss_rpn_box_reg: 0.0757 (0.0757)  time: 1.6478  data: 0.0190  max mem: 0\n",
      "Epoch: [3]  [10/25]  eta: 0:00:22  lr: 0.000500  loss: 1.0439 (0.9724)  loss_classifier: 0.4211 (0.4126)  loss_box_reg: 0.4822 (0.4602)  loss_objectness: 0.0326 (0.0351)  loss_rpn_box_reg: 0.0612 (0.0645)  time: 1.5268  data: 0.0179  max mem: 0\n",
      "Epoch: [3]  [20/25]  eta: 0:00:07  lr: 0.000500  loss: 1.0165 (1.0015)  loss_classifier: 0.4243 (0.4306)  loss_box_reg: 0.4822 (0.4708)  loss_objectness: 0.0374 (0.0383)  loss_rpn_box_reg: 0.0584 (0.0617)  time: 1.4818  data: 0.0174  max mem: 0\n",
      "Epoch: [3]  [24/25]  eta: 0:00:01  lr: 0.000500  loss: 1.0283 (1.0006)  loss_classifier: 0.4386 (0.4288)  loss_box_reg: 0.4841 (0.4753)  loss_objectness: 0.0374 (0.0372)  loss_rpn_box_reg: 0.0542 (0.0593)  time: 1.4374  data: 0.0170  max mem: 0\n",
      "Epoch: [3] Total time: 0:00:36 (1.4717 s / it)\n",
      "Epoch: [4]  [ 0/25]  eta: 0:00:36  lr: 0.000500  loss: 0.7806 (0.7806)  loss_classifier: 0.3780 (0.3780)  loss_box_reg: 0.3129 (0.3129)  loss_objectness: 0.0350 (0.0350)  loss_rpn_box_reg: 0.0547 (0.0547)  time: 1.4622  data: 0.0150  max mem: 0\n",
      "Epoch: [4]  [10/25]  eta: 0:00:21  lr: 0.000500  loss: 1.0502 (1.0313)  loss_classifier: 0.4547 (0.4464)  loss_box_reg: 0.4918 (0.5001)  loss_objectness: 0.0366 (0.0376)  loss_rpn_box_reg: 0.0426 (0.0472)  time: 1.4551  data: 0.0182  max mem: 0\n",
      "Epoch: [4]  [20/25]  eta: 0:00:07  lr: 0.000500  loss: 1.0786 (1.0906)  loss_classifier: 0.4697 (0.4709)  loss_box_reg: 0.5302 (0.5328)  loss_objectness: 0.0366 (0.0373)  loss_rpn_box_reg: 0.0430 (0.0496)  time: 1.4049  data: 0.0178  max mem: 0\n",
      "Epoch: [4]  [24/25]  eta: 0:00:01  lr: 0.000500  loss: 1.0820 (1.0859)  loss_classifier: 0.4838 (0.4763)  loss_box_reg: 0.5290 (0.5223)  loss_objectness: 0.0366 (0.0371)  loss_rpn_box_reg: 0.0435 (0.0502)  time: 1.3439  data: 0.0172  max mem: 0\n",
      "Epoch: [4] Total time: 0:00:34 (1.3647 s / it)\n",
      "Epoch: [5]  [ 0/25]  eta: 0:00:31  lr: 0.000500  loss: 1.1706 (1.1706)  loss_classifier: 0.5306 (0.5306)  loss_box_reg: 0.5686 (0.5686)  loss_objectness: 0.0268 (0.0268)  loss_rpn_box_reg: 0.0446 (0.0446)  time: 1.2718  data: 0.0150  max mem: 0\n",
      "Epoch: [5]  [10/25]  eta: 0:00:19  lr: 0.000500  loss: 1.1109 (1.0968)  loss_classifier: 0.4778 (0.4758)  loss_box_reg: 0.5285 (0.5395)  loss_objectness: 0.0294 (0.0334)  loss_rpn_box_reg: 0.0433 (0.0481)  time: 1.3008  data: 0.0180  max mem: 0\n",
      "Epoch: [5]  [20/25]  eta: 0:00:06  lr: 0.000500  loss: 1.1158 (1.1426)  loss_classifier: 0.4806 (0.4846)  loss_box_reg: 0.5624 (0.5722)  loss_objectness: 0.0334 (0.0344)  loss_rpn_box_reg: 0.0495 (0.0514)  time: 1.3012  data: 0.0173  max mem: 0\n",
      "Epoch: [5]  [24/25]  eta: 0:00:01  lr: 0.000500  loss: 1.1346 (1.1406)  loss_classifier: 0.4814 (0.4847)  loss_box_reg: 0.5683 (0.5710)  loss_objectness: 0.0332 (0.0343)  loss_rpn_box_reg: 0.0498 (0.0506)  time: 1.2596  data: 0.0158  max mem: 0\n",
      "Epoch: [5] Total time: 0:00:31 (1.2696 s / it)\n",
      "Epoch: [6]  [ 0/25]  eta: 0:00:32  lr: 0.000050  loss: 1.1866 (1.1866)  loss_classifier: 0.4977 (0.4977)  loss_box_reg: 0.6264 (0.6264)  loss_objectness: 0.0284 (0.0284)  loss_rpn_box_reg: 0.0341 (0.0341)  time: 1.3007  data: 0.0190  max mem: 0\n",
      "Epoch: [6]  [10/25]  eta: 0:00:19  lr: 0.000050  loss: 1.1106 (1.1334)  loss_classifier: 0.4837 (0.4846)  loss_box_reg: 0.5352 (0.5617)  loss_objectness: 0.0339 (0.0363)  loss_rpn_box_reg: 0.0462 (0.0508)  time: 1.2732  data: 0.0197  max mem: 0\n",
      "Epoch: [6]  [20/25]  eta: 0:00:06  lr: 0.000050  loss: 1.1243 (1.1432)  loss_classifier: 0.4682 (0.4871)  loss_box_reg: 0.5729 (0.5707)  loss_objectness: 0.0346 (0.0352)  loss_rpn_box_reg: 0.0464 (0.0502)  time: 1.2824  data: 0.0187  max mem: 0\n",
      "Epoch: [6]  [24/25]  eta: 0:00:01  lr: 0.000050  loss: 1.1935 (1.1670)  loss_classifier: 0.4793 (0.4971)  loss_box_reg: 0.6027 (0.5857)  loss_objectness: 0.0339 (0.0351)  loss_rpn_box_reg: 0.0475 (0.0490)  time: 1.2600  data: 0.0182  max mem: 0\n",
      "Epoch: [6] Total time: 0:00:31 (1.2575 s / it)\n",
      "Epoch: [7]  [ 0/25]  eta: 0:00:30  lr: 0.000050  loss: 1.1293 (1.1293)  loss_classifier: 0.5015 (0.5015)  loss_box_reg: 0.5513 (0.5513)  loss_objectness: 0.0205 (0.0205)  loss_rpn_box_reg: 0.0560 (0.0560)  time: 1.2383  data: 0.0140  max mem: 0\n",
      "Epoch: [7]  [10/25]  eta: 0:00:18  lr: 0.000050  loss: 1.1405 (1.2163)  loss_classifier: 0.4928 (0.4987)  loss_box_reg: 0.6178 (0.6322)  loss_objectness: 0.0302 (0.0313)  loss_rpn_box_reg: 0.0560 (0.0540)  time: 1.2455  data: 0.0164  max mem: 0\n",
      "Epoch: [7]  [20/25]  eta: 0:00:06  lr: 0.000050  loss: 1.1370 (1.1425)  loss_classifier: 0.4731 (0.4831)  loss_box_reg: 0.5905 (0.5738)  loss_objectness: 0.0319 (0.0346)  loss_rpn_box_reg: 0.0473 (0.0510)  time: 1.2384  data: 0.0166  max mem: 0\n",
      "Epoch: [7]  [24/25]  eta: 0:00:01  lr: 0.000050  loss: 1.1370 (1.1520)  loss_classifier: 0.4734 (0.4864)  loss_box_reg: 0.5879 (0.5823)  loss_objectness: 0.0319 (0.0350)  loss_rpn_box_reg: 0.0432 (0.0482)  time: 1.2042  data: 0.0167  max mem: 0\n",
      "Epoch: [7] Total time: 0:00:30 (1.2130 s / it)\n",
      "Epoch: [8]  [ 0/25]  eta: 0:00:31  lr: 0.000050  loss: 1.0930 (1.0930)  loss_classifier: 0.5035 (0.5035)  loss_box_reg: 0.5048 (0.5048)  loss_objectness: 0.0429 (0.0429)  loss_rpn_box_reg: 0.0418 (0.0418)  time: 1.2703  data: 0.0170  max mem: 0\n",
      "Epoch: [8]  [10/25]  eta: 0:00:18  lr: 0.000050  loss: 1.1503 (1.1600)  loss_classifier: 0.4965 (0.4915)  loss_box_reg: 0.5915 (0.5855)  loss_objectness: 0.0409 (0.0350)  loss_rpn_box_reg: 0.0418 (0.0480)  time: 1.2347  data: 0.0171  max mem: 0\n",
      "Epoch: [8]  [20/25]  eta: 0:00:06  lr: 0.000050  loss: 1.1503 (1.1604)  loss_classifier: 0.4873 (0.4882)  loss_box_reg: 0.5915 (0.5885)  loss_objectness: 0.0322 (0.0356)  loss_rpn_box_reg: 0.0424 (0.0481)  time: 1.2426  data: 0.0172  max mem: 0\n",
      "Epoch: [8]  [24/25]  eta: 0:00:01  lr: 0.000050  loss: 1.1503 (1.1574)  loss_classifier: 0.4873 (0.4887)  loss_box_reg: 0.5793 (0.5855)  loss_objectness: 0.0320 (0.0347)  loss_rpn_box_reg: 0.0446 (0.0485)  time: 1.2166  data: 0.0166  max mem: 0\n",
      "Epoch: [8] Total time: 0:00:30 (1.2234 s / it)\n",
      "Epoch: [9]  [ 0/25]  eta: 0:00:31  lr: 0.000005  loss: 1.1124 (1.1124)  loss_classifier: 0.4903 (0.4903)  loss_box_reg: 0.5512 (0.5512)  loss_objectness: 0.0290 (0.0290)  loss_rpn_box_reg: 0.0419 (0.0419)  time: 1.2450  data: 0.0230  max mem: 0\n",
      "Epoch: [9]  [10/25]  eta: 0:00:18  lr: 0.000005  loss: 1.2313 (1.2171)  loss_classifier: 0.4860 (0.4981)  loss_box_reg: 0.6469 (0.6410)  loss_objectness: 0.0290 (0.0303)  loss_rpn_box_reg: 0.0419 (0.0477)  time: 1.2652  data: 0.0183  max mem: 0\n",
      "Epoch: [9]  [20/25]  eta: 0:00:06  lr: 0.000005  loss: 1.1400 (1.1516)  loss_classifier: 0.4795 (0.4871)  loss_box_reg: 0.5689 (0.5838)  loss_objectness: 0.0303 (0.0326)  loss_rpn_box_reg: 0.0461 (0.0481)  time: 1.2666  data: 0.0179  max mem: 0\n",
      "Epoch: [9]  [24/25]  eta: 0:00:01  lr: 0.000005  loss: 1.1400 (1.1552)  loss_classifier: 0.4829 (0.4872)  loss_box_reg: 0.5328 (0.5873)  loss_objectness: 0.0312 (0.0328)  loss_rpn_box_reg: 0.0461 (0.0479)  time: 1.2505  data: 0.0180  max mem: 0\n",
      "Epoch: [9] Total time: 0:00:31 (1.2537 s / it)\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, \n",
    "                            lr=0.005,\n",
    "                            momentum=0.9, \n",
    "                            weight_decay=0.0005\n",
    "                            )\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(\n",
    "                    model, \n",
    "                    optimizer, \n",
    "                    data_loader, \n",
    "                    device, \n",
    "                    epoch, \n",
    "                    print_freq=10\n",
    "                    )\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights/ocr-gantt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_image(image_file):\n",
    "    model.eval()\n",
    "    img = Image.open(image_file).convert(\"RGB\")\n",
    "    transform = get_transform(train=False)\n",
    "    img, _ = transform(img, None)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    outputs = model(img)\n",
    "    outputs = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in outputs]\n",
    "    \n",
    "    boxes = outputs[0]['boxes'].detach().numpy()\n",
    "    labels = outputs[0]['labels'].detach().numpy()\n",
    "    scores = outputs[0]['scores'].detach().numpy()\n",
    "\n",
    "    img = cv.imread(image_file)\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > 0.5:\n",
    "            x1, y1, x2, y2 = box\n",
    "            cv.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "            cv.putText(img, dataset.classes[label], (int(x1), int(y1)), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv.imwrite('output.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image('data/ocr-od/GC_1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
